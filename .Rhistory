# Remove stop words
jobInfo$text = sapply(jobInfo$text, function(x){
paste(setdiff(strsplit(x," ")[[1]],stopwords()),collapse=" ")
})
# Stem words (Also try without stemming?)
jobInfo$text = sapply(jobInfo$text, function(x)  {
paste(setdiff(wordStem(strsplit(x," ")[[1]]),""),collapse=" ")
})
jobInfo$text
jobCorpus = Corpus(VectorSource(jobInfo$text))
# Create Document Term Matrix
jobDTM = DocumentTermMatrix(jobCorpus)
# Create Term Frequency Matrix
jobFreq = as.matrix(jobDTM)
jobFreq
WCDtm <- TermDocumentMatrix(jobCorpus, control = list(minWordLength = 1))
WCDtm
library(RTextTools)
data(NYTimes)
data <- NYTimes[sample(1:3100,size=100,replace=FALSE),]
matrix <- create_matrix(cbind(data["Title"],data["Subject"]), language="english",
removeNumbers=TRUE, stemWords=FALSE, weighting=tm::weightTfIdf)
container <- create_container(matrix,data$Topic.Code,trainSize=1:75, testSize=76:100,
virgin=FALSE)
models <- train_models(container, algorithms=c("MAXENT","SVM"))
results <- classify_models(container, models)
analytics <- create_analytics(container, results)
summary(analytics)
data
View(data)
matrix
data$Topic.Code
results <- classify_models(container, models)
analytics <- create_analytics(container, results)
summary(analytics)
library(RTextTools)
data(NYTimes)
data <- NYTimes[sample(1:3100,size=100,replace=FALSE),]
matrix <- create_matrix(cbind(data["Title"],data["Subject"]), language="english",
removeNumbers=TRUE, stemWords=FALSE, weighting=tm::weightTfIdf)
matrix
install.packages(c("RTextTools","topicmodels"))
install.packages(c("RTextTools", "topicmodels"))
findFreqTerms(myDtm, lowfreq=50);
findFreqTerms(jobDTM, lowfreq=50);
findFreqTerms(jobDTM, lowfreq=50)
library(topicmodels)
install.packages('library(topicmodels)')
install.packages('topicmodels')
library(topicmodels)
install.packages(c("RTextTools","topicmodels"))
install.packages(c("RTextTools", "topicmodels"))
?prod
?which
setwd("~/CapitalOne_Challenge")
setwd("~/Documents/AerialIntel")
wheat.2013<-read.csv('wheat-2013-supervised.csv')
wheat.2014<-read.csv('wheat-2014-supervised.csv')
#Check number of NAs and other descriptives to get a sense of data
summary(wheat.2013)
summary(wheat.2014)
#Combining data from 2013 and 2014
wheat.df<-rbind(wheat.2014,wheat.2013)
summary(wheat.df)
#Removing features that are to be removed based on the question!
wheat.df$CountyName=NULL
wheat.df$State=NULL
wheat.df$Date=NULL
wheat.df$Latitude=NULL
wheat.df$Longitude=NULL
#Missing values Percentage calculation
sum(is.na(wheat.df))/prod(dim(wheat.df))*100
#Missing values are < 1% so we can neglect the imputation of the missingvalues as we need not induce bias via imputed values.
wheat.df<-na.omit((wheat.df))
#Normalize data to make all feature equally important independant of their ranges
trainIndex <- createDataPartition(wheat.df$Yield, p = .8, list = FALSE, times = 1)
wheat.df_Train <- wheat.df[ trainIndex,]
wheat.df_Test  <- wheat.df[-trainIndex,]
library(caret)
trainIndex <- createDataPartition(wheat.df$Yield, p = .8, list = FALSE, times = 1)
wheat.df_Train <- wheat.df[ trainIndex,]
wheat.df_Test  <- wheat.df[-trainIndex,]
h2o.init(nthreads=-1,max_mem_size = "10G") ## -1: use all available threads
library(h2o)
h2o.init(nthreads=-1,max_mem_size = "10G") ## -1: use all available threads
h2o.removeAll() # Clean slate - just in case the cluster was already running
h2o.train<-as.h2o(wheat.df_Train)
h2o.test<-as.h2o(wheat.df_Test)
y <- c("Yield")
x <- setdiff(names(h2o.train), y)
#first predictive model
rf1 <- h2o.randomForest(
training_frame = h2o.train,              ## the H2O frame for training
x=x,                                     ## the predictor columns, by column index
y=y,                                     ## the target index (what we are predicting)
model_id = "rf_covType_v1",              ## name the model in H2O
ntrees = 100,                            ## use a maximum of 200 trees to create the model
score_each_iteration = F,
nfolds = 6,
seed = 1000000,
stopping_rounds = 3,
stopping_tolerance = 0.001)
Rf_predictions<-h2o.predict(object = rf1,newdata = h2o.test)
performace<-h2o.performance(rf1,newdata = h2o.test)
h2o.mse(performace)
mean((wheat.df_Test$Yield - wheat.df_Test$YieldPredicted)^2)
performace<-h2o.performance(rf1,newdata = h2o.test)
h2o.mse(performace)
wheat.df_Test$YieldPredicted=Rf_predictions
mean((wheat.df_Test$Yield - wheat.df_Test$YieldPredicted)^2)
wheat.df_Train <- as.data.frame(scale(wheat.df_Train))
wheat.df_Test <- as.data.frame(scale(wheat.df_Test))
setwd("~/Documents/AerialIntel")
#Read CSV file
wheat.2013<-read.csv('wheat-2013-supervised.csv')
wheat.2014<-read.csv('wheat-2014-supervised.csv')
#Check number of NAs and other descriptives to get a sense of data
summary(wheat.2013)
summary(wheat.2014)
#Combining data from 2013 and 2014
wheat.df<-rbind(wheat.2014,wheat.2013)
summary(wheat.df)
wheat.df$CountyName=NULL
wheat.df$State=NULL
wheat.df$Date=NULL
wheat.df$Latitude=NULL
wheat.df$Longitude=NULL
#Missing values Percentage calculation
sum(is.na(wheat.df))/prod(dim(wheat.df))*100
#Missing values are < 1% so we can neglect the imputation of the missingvalues as we need not induce bias via imputed values.
wheat.df<-na.omit((wheat.df))
#Exploratory data analysis
plot()
#split Train and Test dataset
set.seed(3456)
trainIndex <- createDataPartition(wheat.df$Yield, p = .8, list = FALSE, times = 1)
wheat.df_Train <- wheat.df[ trainIndex,]
wheat.df_Test  <- wheat.df[-trainIndex,]
#Scale data to make all feature equally important independant of their ranges
#Scale after splitting to avoid data snooping
wheat.df_Train <- as.data.frame(scale(wheat.df_Train))
wheat.df_Test <- as.data.frame(scale(wheat.df_Test))
#Using H2o package for the predictive modelling as it has capabilities to run the models in a
#distributed fashion on the JVM using in memory processing.
## Create an H2O cloud
h2o.init(nthreads=-1,max_mem_size = "10G") ## -1: use all available threads
h2o.removeAll() # Clean slate - just in case the cluster was already running
h2o.train<-as.h2o(wheat.df_Train)
h2o.test<-as.h2o(wheat.df_Test)
head(h2o.train)
y <- c("Yield")
x <- setdiff(names(h2o.train), y)
#first predictive model
rf1 <- h2o.randomForest(
training_frame = h2o.train,              ## the H2O frame for training
x=x,                                     ## the predictor columns, by column index
y=y,                                     ## the target index (what we are predicting)
model_id = "rf_covType_v1",              ## name the model in H2O
ntrees = 150,                            ## use a maximum of 200 trees to create the model
score_each_iteration = F,
seed = 1000000,
stopping_rounds = 3,
stopping_tolerance = 0.001)
Rf_predictions<-h2o.predict(object = rf1,newdata = h2o.test)
performace<-h2o.performance(rf1,newdata = h2o.test)
h2o.mse(performace)
wheat.df_Test$YieldPredicted=Rf_predictions
mean((wheat.df_Test$Yield - wheat.df_Test$YieldPredicted)^2)
mean((wheat.df_Test$Yield )
mean((wheat.df_Test$Yield ))
mean(((wheat.df_Test$Yield) - ((wheat.df_Test$YieldPredicted)^2)))
a=wheat.df_Test$Yield
b=wheat.df_Test$YieldPredicted
mean(((a) - (b))^2))
mean((a - b)^2)
mean((a - b)^2)
a-b
rmse(a,b)
library(Metrics)
rmse(a,b)
type(a)
typeof(a)
a=as.vector(wheat.df_Test$Yield)
b=as.vector(wheat.df_Test$YieldPredicted)
rmse(a,b)
rmse(as.vector(wheat.df_Test$Yield),as.vector(wheat.df_Test$YieldPredicted))
gbm1 <- h2o.gbm(
training_frame = h2o.train,     ##
x=x,                     ##
y=y,
nfolds = 3,
stopping_metric = "MSE",
ntrees = 100,                ## decrease the trees, mostly to allow for run time
learn_rate = 0.2,           ## increase the learning rate (from 0.1)
max_depth = 60,             ## increase the depth (from 5)
stopping_rounds = 3,        ##
stopping_tolerance = 0.01,  ##
score_each_iteration = T,   ##
model_id = "gbm_covType2",  ##
seed = 20000
)
gbm_predictions<-h2o.predict(
object = gbm1,newdata = h2o.test)
h2o.performance(gbm1,newdata = h2o.test)
rmse(as.vector(wheat.df_Test$Yield),as.vector(gbm_predictions))
Dl_model <- h2o.deeplearning(x = x, y = y , model_id = "dll",training_frame = h2o.train,activation ='RectifierWithDropout' ,epochs =100 ,hidden = c(150,50),l1=1e-3 )
dl_predictions<-h2o.predict(
object = Dl_model,newdata = h2o.test)
h2o.performance(Dl_model,newdata = h2o.test)
Dl_model <- h2o.deeplearning(x = x, y = y , model_id = "dll",training_frame = h2o.train,activation ='RectifierWithDropout',
epochs =100 ,hidden = c(150,100),l1=1e-5 )
dl_predictions<-h2o.predict(
object = Dl_model,newdata = h2o.test)
h2o.performance(Dl_model,newdata = h2o.test)
Dl_model <- h2o.deeplearning(x = x, y = y , model_id = "dll",training_frame = h2o.train,activation ='RectifierWithDropout',
epochs =100 ,hidden = c(200,200,150,100),l1=1e-5 )
learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper",
"h2o.gbm.wrapper")
metalearner <- "h2o.gbm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
library(h2oEnsemble)
install.packages("https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.8.tar.gz", repos = NULL)
library(h2oEnsemble)
learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper",
"h2o.gbm.wrapper")
metalearner <- "h2o.gbm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
fit
h2o.performance(fit,newdata = h2o.test)
pred <- predict(fit, h2o.test)
pred
rmse(as.vector(pred$pred),wheat.df_Test$Yield)
learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper",
"h2o.gbm.wrapper")
metalearner <- "h2o.glm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 2, shuffle = TRUE))
pred <- predict(fit, h2o.test)
rmse(as.vector(pred$pred),wheat.df_Test$Yield)
learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper",
"h2o.gbm")
metalearner <- "h2o.gbm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 2, shuffle = TRUE))
pred <- predict(fit, h2o.test)
rmse(as.vector(pred$pred),wheat.df_Test$Yield)
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
pred <- predict(fit, h2o.test)
rmse(as.vector(pred$pred),wheat.df_Test$Yield)
learn_rate_opt <- c(0.01, 0.2)
max_depth_opt <- c(3, 4, 5)
sample_rate_opt <- c(0.7, 0.8, 0.9)
col_sample_rate_opt <- c(0.2, 0.3, 0.4, 0.5, 0.6)
ntrees<-c(200,100)
nfolds <- 3
hyper_params <- list(learn_rate = learn_rate_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt,
col_sample_rate = col_sample_rate_opt,ntrees=ntrees)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = h2o.train,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params
)
learn_rate_opt <- c(0.01, 0.2)
max_depth_opt <- c(3, 4, 5)
sample_rate_opt <- c(0.7, 0.8, 0.9)
col_sample_rate_opt <- c(0.3, 0.4, 0.5, 0.6)
ntrees<-c(200,100)
nfolds <- 0
hyper_params <- list(learn_rate = learn_rate_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt,
col_sample_rate = col_sample_rate_opt,ntrees=ntrees)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = h2o.train,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params
)
h2o.removeAll() # Clean slate - just in case the cluster was already running
learn_rate_opt <- c(0.01, 0.2)
max_depth_opt <- c(3, 4, 5)
sample_rate_opt <- c(0.7, 0.8, 0.9)
col_sample_rate_opt <- c(0.3, 0.4, 0.5, 0.6)
ntrees<-c(200,100)
nfolds <- 0
hyper_params <- list(learn_rate = learn_rate_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt,
col_sample_rate = col_sample_rate_opt,ntrees=ntrees)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = h2o.train,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params
)
h2o.train<-as.h2o(wheat.df_Train)
h2o.test<-as.h2o(wheat.df_Test)
head(h2o.train)
y <- c("Yield")
x <- setdiff(names(h2o.train), y)
#first predictive model
learn_rate_opt <- c(0.01, 0.2)
max_depth_opt <- c(3, 4, 5)
sample_rate_opt <- c(0.7, 0.8, 0.9)
col_sample_rate_opt <- c(0.3, 0.4, 0.5, 0.6)
ntrees<-c(200,100)
nfolds <- 0
hyper_params <- list(learn_rate = learn_rate_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt,
col_sample_rate = col_sample_rate_opt,ntrees=ntrees)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = h2o.train,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params
)
nfolds <- 1
hyper_params <- list(learn_rate = learn_rate_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt,
col_sample_rate = col_sample_rate_opt,ntrees=ntrees)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = h2o.train,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params
)
h2o.init(nthreads=-1,max_mem_size = "10G") ## -1: use all available threads
h2o.removeAll() # Clean slate - just in case the cluster was already running
h2o.train<-as.h2o(wheat.df_Train)
h2o.test<-as.h2o(wheat.df_Test)
head(h2o.train)
y <- c("Yield")
x <- setdiff(names(h2o.train), y)
#first predictive model
h2o.test<-as.h2o(wheat.df_Test)
library(corrplot)
install.packages(corrplot)
install.packages('corrplot')
library(corrplot)
M <- cor(wheat.df_Test)
M <- cor(wheat.df_Train)
corrplot(M, method="circle")
View(wheat.df_Train)
View(wheat.2013)
wheat.df$precipTypeIsOther=NULL  #Constant Column
trainIndex <- createDataPartition(wheat.df$Yield, p = .8, list = FALSE, times = 1)
wheat.df_Train <- wheat.df[ trainIndex,]
wheat.df_Test  <- wheat.df[-trainIndex,]
library(corrplot)
M <- cor(wheat.df_Train)
corrplot(M, method="circle")
corrplot(M, order="hclust", addrect=2)
corrplot(M, order="hclust", addrect=3) # Correlation plot of Hierachial Clusters
library(outliers)
#install.packages('outliers)
install.packages('outliers)
install.packages('outliers')
library(outliers)
data.outliers<-outlier(wheat.df_Train$Yield)
data.outliers
qplot(Yield,data=wheat.df_Train,geom='boxplot', main = 'Box Plot of Tip vs Distance')
View(trainIndex)
qplot(Yield,humidity,data=wheat.df_Train,geom='boxplot', main = 'Box Plot of Tip vs Distance')
qplot(Yield,temperatureMax,data=wheat.df_Train,geom='boxplot', main = 'Box Plot of Tip vs Distance')
plot(wheat.df_Train$temperatureMax)
plot(wheat.df_Train$temperatureMax)
plot(wheat.df_Train$Yield)
plot(wheat.df_Test)
plot(wheat.df_Test[1:1000,])
plot(wheat.df_Test[1:1000,1:9])
qplot(Yield,temperatureMax,data=wheat.df_Train,geom='boxplot', main = 'Box Plot of Tip vs Distance')
qplot(Yield,DayInSeason,data=wheat.df_Train,geom='boxplot', main = 'Box Plot of Tip vs Distance')
qplot(Yield,pressure,data=wheat.df_Train,geom='boxplot', main = 'Box Plot of Tip vs Distance')
Dl_model <- h2o.deeplearning(x = x, y = y , model_id = "dll",training_frame = h2o.train,activation ='RectifierWithDropout',
epochs =200 ,hidden = c(200,200,150),l1=1e-5 )
Dl_model <- h2o.deeplearning(x = x, y = y , model_id = "dll",training_frame = h2o.train,activation ='RectifierWithDropout',
epochs =200 ,hidden = c(200,200,150) )
learner <- c("h2o.glm.wrapper", "h2o.rf",
"h2o.gbm")
metalearner <- "h2o.gbm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
library(h2oEnsemble)
learner <- c("h2o.glm.wrapper", "h2o.rf",
"h2o.gbm")
metalearner <- "h2o.gbm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
learner <- c("h2o.glm.wrapper", "h2o.rf",
"h2o.gbm")
metalearner <- "h2o.gbm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
learner <- c("h2o.randomForest.wrapper",
"h2o.gbm")
metalearner <- "h2o.gbm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
learner <- c("h2o.randomForest.wrapper",
"h2o.gbm.wrapper")
metalearner <- "h2o.gbm.wrapper"
# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y,
training_frame = h2o.train,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 5, shuffle = TRUE))
pred <- predict(fit, h2o.test)
wheat.df_Test  <- wheat.df[-trainIndex,]
data.outliers<-outlier(wheat.df_Train$Yield)
h2o.test<-as.h2o(wheat.df_Test)
rmse(as.vector(pred$pred),wheat.df_Test$Yield)
pred <- predict(fit, h2o.test)
rmse(as.vector(pred$pred),wheat.df_Test$Yield)
alpha_opts = list(list(0), list(.25), list(.5), list(.75), list(1))
hyper_parameters = list(alpha = alpha_opts)
grid <- h2o.grid("glm", hyper_params = hyper_parameters,
y = y, x = x,
training_frame = h2o.train)
grid_models <- lapply(grid@model_ids, function(model_id) { model = h2o.
getModel(model_id) })
for (i in 1:length(grid_models)) {
print(sprintf("regularization: %-50s auc: %f", grid_models[[i]]
@model$model_summary$regularization, h2o.auc(grid_models[[i]])))
grid_models <- lapply(grid@model_ids, function(model_id) { model = h2o.getModel(model_id) })
for (i in 1:length(grid_models)) {
print(sprintf("regularization: %-50s auc: %f", grid_models[[i]]
@model$model_summary$regularization, h2o.auc(grid_models[[i]])))
}
}
grid_models <- lapply(grid@model_ids, function(model_id) { model = h2o.getModel(model_id) })
grid_models <- lapply(grid@model_ids, function(model_id) { model = h2o.getModel(model_id) })
for (i in 1:length(grid_models)) {
print(sprintf("regularization: %-50s auc: %f", grid_models[[i]]@model$model_summary$regularization, h2o.auc(grid_models[[i]])))
}
for (i in 1:length(grid_models)) {
print(sprintf("regularization: %-50s auc: %f", grid_models[[i]]@model$model_summary$regularization, h2o.mse(grid_models[[i]])))
}
alpha_opts = c(0,0.25,.4,.6,.8,1)
hyper_parameters = list(alpha = alpha_opts)
grid <- h2o.grid("glm", hyper_params = hyper_parameters,
y = y, x = x,
training_frame = h2o.train)
h2o.glm(y = y, x = x, training_frame = h2o.train, lambda_search = TRUE, max_active_predictors = 10)
h2o.glm(y = y, x = x, training_frame = h2o.train, lambda_search = TRUE, max_active_predictors = 20)
